{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOH4tgCleKQ34s+P1IWwk22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nindzyaa/Autumn_Python_2022/blob/main/homework-6/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RtXPyhuxRZne"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import LinAlgError\n",
        "import scipy\n",
        "from datetime import datetime\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class LineSearchTool(object):\n",
        "    \"\"\"\n",
        "    Line search tool for adaptively tuning the step size of the algorithm.\n",
        "    method : String containing 'Wolfe', 'Armijo' or 'Constant'\n",
        "        Method of tuning step-size.\n",
        "        Must be be one of the following strings:\n",
        "            - 'Wolfe' -- enforce strong Wolfe conditions;\n",
        "            - 'Armijo\" -- adaptive Armijo rule;\n",
        "            - 'Constant' -- constant step size.\n",
        "    kwargs :\n",
        "        Additional parameters of line_search method:\n",
        "        If method == 'Wolfe':\n",
        "            c1, c2 : Constants for strong Wolfe conditions\n",
        "            alpha_0 : Starting point for the backtracking procedure\n",
        "                to be used in Armijo method in case of failure of Wolfe method.\n",
        "        If method == 'Armijo':\n",
        "            c1 : Constant for Armijo rule\n",
        "            alpha_0 : Starting point for the backtracking procedure.\n",
        "        If method == 'Constant':\n",
        "            c : The step size which is returned on every step.\n",
        "    \"\"\"\n",
        "    def __init__(self, method='Wolfe', **kwargs):\n",
        "        self._method = method\n",
        "        if self._method == 'Wolfe':\n",
        "            self.c1 = kwargs.get('c1', 1e-4)\n",
        "            self.c2 = kwargs.get('c2', 0.9)\n",
        "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
        "        elif self._method == 'Armijo':\n",
        "            self.c1 = kwargs.get('c1', 1e-4)\n",
        "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
        "        elif self._method == 'Constant':\n",
        "            self.c = kwargs.get('c', 1.0)\n",
        "        else:\n",
        "            raise ValueError('Unknown method {}'.format(method))\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, options):\n",
        "        if type(options) != dict:\n",
        "            raise TypeError('LineSearchTool initializer must be of type dict')\n",
        "        return cls(**options)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return self.__dict__\n",
        "\n",
        "    def line_search(self, oracle, x_k, d_k, previous_alpha=None):\n",
        "        \"\"\"\n",
        "        Finds the step size alpha for a given starting point x_k\n",
        "        and for a given search direction d_k that satisfies necessary\n",
        "        conditions for phi(alpha) = oracle.func(x_k + alpha * d_k).\n",
        "        Parameters\n",
        "        ----------\n",
        "        oracle : BaseSmoothOracle-descendant object\n",
        "            Oracle with .func_directional() and .grad_directional() methods implemented for computing\n",
        "            function values and its directional derivatives.\n",
        "        x_k : np.array\n",
        "            Starting point\n",
        "        d_k : np.array\n",
        "            Search direction\n",
        "        previous_alpha : float or None\n",
        "            Starting point to use instead of self.alpha_0 to keep the progress from\n",
        "             previous steps. If None, self.alpha_0, is used as a starting point.\n",
        "        Returns\n",
        "        -------\n",
        "        alpha : float or None if failure\n",
        "            Chosen step size\n",
        "        \"\"\"\n",
        "\n",
        "        phi = lambda alpha: oracle.func(x_k + alpha * d_k)\n",
        "        derphi = lambda alpha: oracle.grad(x_k + alpha * d_k).dot(d_k)\n",
        "        if self._method == \"Wolfe\":\n",
        "            result_alpha = scipy.optimize.linesearch.scalar_search_wolfe2(phi, derphi, c1=self.c1, c2=self.c2)[0]\n",
        "            if result_alpha == None:\n",
        "                return LineSearchTool(method='Armijo', c1=self.c1, alpha_0=self.alpha_0).line_search(oracle, x_k, d_k, previous_alpha)\n",
        "        elif self._method == \"Armijo\":\n",
        "            if previous_alpha == None:\n",
        "                previous_alpha = self.alpha_0\n",
        "            alpha = previous_alpha\n",
        "            while phi(alpha) > phi(0) + self.c1 * alpha * derphi(0):\n",
        "                alpha = alpha / 2\n",
        "            result_alpha = alpha\n",
        "        else:\n",
        "            result_alpha = self.c\n",
        "        return result_alpha\n",
        "\n",
        "\n",
        "def get_line_search_tool(line_search_options=None):\n",
        "    if line_search_options:\n",
        "        if type(line_search_options) is LineSearchTool:\n",
        "            return line_search_options\n",
        "        else:\n",
        "            return LineSearchTool.from_dict(line_search_options)\n",
        "    else:\n",
        "        return LineSearchTool()\n",
        "\n",
        "def update(trace, display, history, oracle, time, x_k, it):\n",
        "    if display:\n",
        "        print(\"Iteration \", it, \": x_k = \", x_k, sep = '')\n",
        "    if trace:\n",
        "        history['time'].append(time)\n",
        "        history['func'].append(oracle.func(x_k))\n",
        "        history['grad_norm'].append(np.linalg.norm(oracle.grad(x_k)))\n",
        "        if len(x_k) <= 2:\n",
        "            history['x'].append(np.copy(x_k))\n",
        "    return history\n",
        "\n",
        "def gradient_descent(oracle, x_0, tolerance=1e-5, max_iter=10000,\n",
        "                     line_search_options=None, trace=False, display=False):\n",
        "    \"\"\"\n",
        "    Gradien descent optimization method.\n",
        "    Parameters\n",
        "    ----------\n",
        "    oracle : BaseSmoothOracle-descendant object\n",
        "        Oracle with .func(), .grad() and .hess() methods implemented for computing\n",
        "        function value, its gradient and Hessian respectively.\n",
        "    x_0 : np.array\n",
        "        Starting point for optimization algorithm\n",
        "    tolerance : float\n",
        "        Epsilon value for stopping criterion.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations.\n",
        "    line_search_options : dict, LineSearchTool or None\n",
        "        Dictionary with line search options. See LineSearchTool class for details.\n",
        "    trace : bool\n",
        "        If True, the progress information is appended into history dictionary during training.\n",
        "        Otherwise None is returned instead of history.\n",
        "    display : bool\n",
        "        If True, debug information is displayed during optimization.\n",
        "        Printing format and is up to a student and is not checked in any way.\n",
        "    Returns\n",
        "    -------\n",
        "    x_star : np.array\n",
        "        The point found by the optimization procedure\n",
        "    message : string\n",
        "        \"success\" or the description of error:\n",
        "            - 'iterations_exceeded': if after max_iter iterations of the method x_k still doesn't satisfy\n",
        "                the stopping criterion.\n",
        "            - 'computational_error': in case of getting Infinity or None value during the computations.\n",
        "    history : dictionary of lists or None\n",
        "        Dictionary containing the progress information or None if trace=False.\n",
        "        Dictionary has to be organized as follows:\n",
        "            - history['time'] : list of floats, containing time in seconds passed from the start of the method\n",
        "            - history['func'] : list of function values f(x_k) on every step of the algorithm\n",
        "            - history['grad_norm'] : list of values Euclidian norms ||g(x_k)|| of the gradient on every step of the algorithm\n",
        "            - history['x'] : list of np.arrays, containing the trajectory of the algorithm. ONLY STORE IF x.size <= 2\n",
        "    Example:\n",
        "    --------\n",
        "    >> oracle = QuadraticOracle(np.eye(5), np.arange(5))\n",
        "    >> x_opt, message, history = gradient_descent(oracle, np.zeros(5), line_search_options={'method': 'Armijo', 'c1': 1e-4})\n",
        "    >> print('Found optimal point: {}'.format(x_opt))\n",
        "       Found optimal point: [ 0.  1.  2.  3.  4.]\n",
        "    \"\"\"\n",
        "    history = defaultdict(list) if trace else None\n",
        "    line_search_tool = get_line_search_tool(line_search_options)\n",
        "    x_k = np.copy(x_0)\n",
        "    it = 0\n",
        "    start_time = time.time()\n",
        "    history = update(trace, display, history, oracle, 0, x_k, it)\n",
        "\n",
        "    while np.linalg.norm(oracle.grad(x_k)) ** 2 >= tolerance and it <= max_iter:\n",
        "        d_k = -oracle.grad(x_k)\n",
        "        alpha_k = line_search_tool.line_search(oracle, x_k, d_k)\n",
        "        x_k += alpha_k * d_k\n",
        "        if (None in x_k) or (x_k > 10**9).any():\n",
        "            return x_k, 'computational_error', history\n",
        "        it += 1\n",
        "        history = update(trace, display, history, oracle, time.time() - start_time, x_k, it)\n",
        "        \n",
        "        if it > max_iter:\n",
        "            return x_k, 'iterations_exceeded', history\n",
        "\n",
        "    return x_k, 'success', history\n",
        "\n",
        "\n",
        "def newton(oracle, x_0, tolerance=1e-5, max_iter=100,\n",
        "           line_search_options=None, trace=False, display=False):\n",
        "    \"\"\"\n",
        "    Newton's optimization method.\n",
        "    Parameters\n",
        "    ----------\n",
        "    oracle : BaseSmoothOracle-descendant object\n",
        "        Oracle with .func(), .grad() and .hess() methods implemented for computing\n",
        "        function value, its gradient and Hessian respectively. If the Hessian\n",
        "        returned by the oracle is not positive-definite method stops with message=\"newton_direction_error\"\n",
        "    x_0 : np.array\n",
        "        Starting point for optimization algorithm\n",
        "    tolerance : float\n",
        "        Epsilon value for stopping criterion.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations.\n",
        "    line_search_options : dict, LineSearchTool or None\n",
        "        Dictionary with line search options. See LineSearchTool class for details.\n",
        "    trace : bool\n",
        "        If True, the progress information is appended into history dictionary during training.\n",
        "        Otherwise None is returned instead of history.\n",
        "    display : bool\n",
        "        If True, debug information is displayed during optimization.\n",
        "    Returns\n",
        "    -------\n",
        "    x_star : np.array\n",
        "        The point found by the optimization procedure\n",
        "    message : string\n",
        "        'success' or the description of error:\n",
        "            - 'iterations_exceeded': if after max_iter iterations of the method x_k still doesn't satisfy\n",
        "                the stopping criterion.\n",
        "            - 'newton_direction_error': in case of failure of solving linear system with Hessian matrix (e.g. non-invertible matrix).\n",
        "            - 'computational_error': in case of getting Infinity or None value during the computations.\n",
        "    history : dictionary of lists or None\n",
        "        Dictionary containing the progress information or None if trace=False.\n",
        "        Dictionary has to be organized as follows:\n",
        "            - history['time'] : list of floats, containing time passed from the start of the method\n",
        "            - history['func'] : list of function values f(x_k) on every step of the algorithm\n",
        "            - history['grad_norm'] : list of values Euclidian norms ||g(x_k)|| of the gradient on every step of the algorithm\n",
        "            - history['x'] : list of np.arrays, containing the trajectory of the algorithm. ONLY STORE IF x.size <= 2\n",
        "    Example:\n",
        "    --------\n",
        "    >> oracle = QuadraticOracle(np.eye(5), np.arange(5))\n",
        "    >> x_opt, message, history = newton(oracle, np.zeros(5), line_search_options={'method': 'Constant', 'c': 1.0})\n",
        "    >> print('Found optimal point: {}'.format(x_opt))\n",
        "       Found optimal point: [ 0.  1.  2.  3.  4.]\n",
        "    \"\"\"\n",
        "    history = defaultdict(list) if trace else None\n",
        "    line_search_tool = get_line_search_tool(line_search_options)\n",
        "    x_k = np.copy(x_0)\n",
        "    it = 0\n",
        "    start_time = time.time()\n",
        "    history = update(trace, display, history, oracle, 0, x_k, it)\n",
        "\n",
        "    while np.linalg.norm(oracle.grad(x_k)) ** 2 >= tolerance and it <= max_iter:        \n",
        "        try:\n",
        "            c, low = scipy.linalg.cho_factor(oracle.hess(x_k))\n",
        "            d_k = scipy.linalg.cho_solve((c, low), -oracle.grad(x_k))\n",
        "        except LinAlgError:\n",
        "            return x_k, 'newton_direction_error', history\n",
        "\n",
        "        alpha_k = line_search_tool.line_search(oracle, x_k, d_k)\n",
        "        x_k += alpha_k * d_k\n",
        "        if (None in x_k) or (x_k > 10**9).any():\n",
        "            return x_k, 'computational_error', history\n",
        "        it += 1\n",
        "        history = update(trace, display, history, oracle, time.time() - start_time, x_k, it)\n",
        "\n",
        "    if it > max_iter:\n",
        "        return x_k, 'iterations_exceeded', history\n",
        "\n",
        "    return x_k, 'success', history"
      ]
    }
  ]
}