{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrTlAjtp++BVDrrEjH1bLY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nindzyaa/Autumn_Python_2022/blob/main/homework-6/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import LinAlgError\n",
        "import scipy\n",
        "from time import time\n",
        "from collections import defaultdict\n",
        "\n",
        "from scipy.optimize.linesearch import scalar_search_wolfe2\n",
        "\n",
        "\n",
        "class LineSearchTool(object):\n",
        "    \"\"\"\n",
        "    Line search tool for adaptively tuning the step size of the algorithm.\n",
        "    method : String containing 'Wolfe', 'Armijo' or 'Constant'\n",
        "        Method of tuning step-size.\n",
        "        Must be be one of the following strings:\n",
        "            - 'Wolfe' -- enforce strong Wolfe conditions;\n",
        "            - 'Armijo\" -- adaptive Armijo rule;\n",
        "            - 'Constant' -- constant step size.\n",
        "    kwargs :\n",
        "        Additional parameters of line_search method:\n",
        "        If method == 'Wolfe':\n",
        "            c1, c2 : Constants for strong Wolfe conditions\n",
        "            alpha_0 : Starting point for the backtracking procedure\n",
        "                to be used in Armijo method in case of failure of Wolfe method.\n",
        "        If method == 'Armijo':\n",
        "            c1 : Constant for Armijo rule\n",
        "            alpha_0 : Starting point for the backtracking procedure.\n",
        "        If method == 'Constant':\n",
        "            c : The step size which is returned on every step.\n",
        "    \"\"\"\n",
        "    def __init__(self, method='Wolfe', **kwargs):\n",
        "        self._method = method\n",
        "        if self._method == 'Wolfe':\n",
        "            self.c1 = kwargs.get('c1', 1e-4)\n",
        "            self.c2 = kwargs.get('c2', 0.9)\n",
        "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
        "        elif self._method == 'Armijo':\n",
        "            self.c1 = kwargs.get('c1', 1e-4)\n",
        "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
        "        elif self._method == 'Constant':\n",
        "            self.c = kwargs.get('c', 1.0)\n",
        "        else:\n",
        "            raise ValueError('Unknown method {}'.format(method))\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, options):\n",
        "        if type(options) != dict:\n",
        "            raise TypeError('LineSearchTool initializer must be of type dict')\n",
        "        return cls(**options)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return self.__dict__\n",
        "\n",
        "    def line_search(self, oracle, x_k, d_k, previous_alpha=None):\n",
        "        \"\"\"\n",
        "        Finds the step size alpha for a given starting point x_k\n",
        "        and for a given search direction d_k that satisfies necessary\n",
        "        conditions for phi(alpha) = oracle.func(x_k + alpha * d_k).\n",
        "        Parameters\n",
        "        ----------\n",
        "        oracle : BaseSmoothOracle-descendant object\n",
        "            Oracle with .func_directional() and .grad_directional() methods implemented for computing\n",
        "            function values and its directional derivatives.\n",
        "        x_k : np.array\n",
        "            Starting point\n",
        "        d_k : np.array\n",
        "            Search direction\n",
        "        previous_alpha : float or None\n",
        "            Starting point to use instead of self.alpha_0 to keep the progress from\n",
        "             previous steps. If None, self.alpha_0, is used as a starting point.\n",
        "        Returns\n",
        "        -------\n",
        "        alpha : float or None if failure\n",
        "            Chosen step size\n",
        "        \"\"\"\n",
        "        # TODO: Implement line search procedures for Armijo, Wolfe and Constant steps.\n",
        "        wolfe_lose = False\n",
        "        if self._method == 'Wolfe':\n",
        "            f = lambda alpha: oracle.func_directional(x_k, d_k, alpha)\n",
        "            myfprime = lambda alpha: oracle.grad_directional(x_k, d_k, alpha)\n",
        "            alpha, *_ = scalar_search_wolfe2(f, myfprime, f(0), None, myfprime(0), self.c1, self.c2)\n",
        "            wolfe_lose = alpha is None\n",
        "\n",
        "        if self._method == 'Armijo' or wolfe_lose:\n",
        "            alpha = previous_alpha or self.alpha_0\n",
        "            c = self.c1\n",
        "            phi = lambda alpha: oracle.func_directional(x_k, d_k, alpha)\n",
        "            phi_0 = phi(0)\n",
        "            der_0 = oracle.grad_directional(x_k, d_k, 0)\n",
        "            while phi(alpha) > phi_0 + c * alpha * der_0:\n",
        "                alpha /= 2\n",
        "\n",
        "        if self._method == 'Constant':\n",
        "            alpha = self.c\n",
        "\n",
        "        return alpha\n",
        "\n",
        "\n",
        "def get_line_search_tool(line_search_options=None):\n",
        "    if line_search_options:\n",
        "        if type(line_search_options) is LineSearchTool:\n",
        "            return line_search_options\n",
        "        else:\n",
        "            return LineSearchTool.from_dict(line_search_options)\n",
        "    else:\n",
        "        return LineSearchTool()\n",
        "\n",
        "\n",
        "def gradient_descent(oracle, x_0, tolerance=1e-5, max_iter=10000,\n",
        "                     line_search_options=None, trace=False, display=False):\n",
        "    \"\"\"\n",
        "    Gradient descent optimization method.\n",
        "    Parameters\n",
        "    ----------\n",
        "    oracle : BaseSmoothOracle-descendant object\n",
        "        Oracle with .func(), .grad() and .hess() methods implemented for computing\n",
        "        function value, its gradient and Hessian respectively.\n",
        "    x_0 : np.array\n",
        "        Starting point for optimization algorithm\n",
        "    tolerance : float\n",
        "        Epsilon value for stopping criterion.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations.\n",
        "    line_search_options : dict, LineSearchTool or None\n",
        "        Dictionary with line search options. See LineSearchTool class for details.\n",
        "    trace : bool\n",
        "        If True, the progress information is appended into history dictionary during training.\n",
        "        Otherwise None is returned instead of history.\n",
        "    display : bool\n",
        "        If True, debug information is displayed during optimization.\n",
        "        Printing format and is up to a student and is not checked in any way.\n",
        "    Returns\n",
        "    -------\n",
        "    x_star : np.array\n",
        "        The point found by the optimization procedure\n",
        "    message : string\n",
        "        \"success\" or the description of error:\n",
        "            - 'iterations_exceeded': if after max_iter iterations of the method x_k still doesn't satisfy\n",
        "                the stopping criterion.\n",
        "            - 'computational_error': in case of getting Infinity or None value during the computations.\n",
        "    history : dictionary of lists or None\n",
        "        Dictionary containing the progress information or None if trace=False.\n",
        "        Dictionary has to be organized as follows:\n",
        "            - history['time'] : list of floats, containing time in seconds passed from the start of the method\n",
        "            - history['func'] : list of function values f(x_k) on every step of the algorithm\n",
        "            - history['grad_norm'] : list of values Euclidian norms ||g(x_k)|| of the gradient on every step of the algorithm\n",
        "            - history['x'] : list of np.arrays, containing the trajectory of the algorithm. ONLY STORE IF x.size <= 2\n",
        "    Example:\n",
        "    --------\n",
        "    >> oracle = QuadraticOracle(np.eye(5), np.arange(5))\n",
        "    >> x_opt, message, history = gradient_descent(oracle, np.zeros(5), line_search_options={'method': 'Armijo', 'c1': 1e-4})\n",
        "    >> print('Found optimal point: {}'.format(x_opt))\n",
        "       Found optimal point: [ 0.  1.  2.  3.  4.]\n",
        "    \"\"\"\n",
        "    history = defaultdict(list) if trace else None\n",
        "    line_search_tool = get_line_search_tool(line_search_options)\n",
        "    x_k = np.copy(x_0)\n",
        "    alpha_k = None\n",
        "    time_start = time()\n",
        "    norm_0 = np.linalg.norm(oracle.grad(x_0))\n",
        "    stop = tolerance * (norm_0 ** 2)\n",
        "    for k in range(max_iter):\n",
        "        d_k = -oracle.grad(x_k)\n",
        "        alpha_k = line_search_tool.line_search(oracle, x_k, d_k, 2 * alpha_k if alpha_k else None)\n",
        "        norm = np.linalg.norm(d_k)\n",
        "\n",
        "        chk = check(k, x_k, norm, stop, oracle, history, display, trace, time_start)\n",
        "        if chk:\n",
        "            return chk\n",
        "        x_k = x_k + alpha_k * d_k\n",
        "\n",
        "    chk = check(max_iter, x_k, np.linalg.norm(oracle.grad(x_k)),\n",
        "                stop, oracle, history, display, trace, time_start)\n",
        "    if chk:\n",
        "        return chk\n",
        "    return x_k, 'iterations_exceeded', history\n",
        "\n",
        "\n",
        "def newton(oracle, x_0, tolerance=1e-5, max_iter=100,\n",
        "           line_search_options=None, trace=False, display=False):\n",
        "    \"\"\"\n",
        "    Newton's optimization method.\n",
        "    Parameters\n",
        "    ----------\n",
        "    oracle : BaseSmoothOracle-descendant object\n",
        "        Oracle with .func(), .grad() and .hess() methods implemented for computing\n",
        "        function value, its gradient and Hessian respectively. If the Hessian\n",
        "        returned by the oracle is not positive-definite method stops with message=\"newton_direction_error\"\n",
        "    x_0 : np.array\n",
        "        Starting point for optimization algorithm\n",
        "    tolerance : float\n",
        "        Epsilon value for stopping criterion.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations.\n",
        "    line_search_options : dict, LineSearchTool or None\n",
        "        Dictionary with line search options. See LineSearchTool class for details.\n",
        "    trace : bool\n",
        "        If True, the progress information is appended into history dictionary during training.\n",
        "        Otherwise None is returned instead of history.\n",
        "    display : bool\n",
        "        If True, debug information is displayed during optimization.\n",
        "    Returns\n",
        "    -------\n",
        "    x_star : np.array\n",
        "        The point found by the optimization procedure\n",
        "    message : string\n",
        "        'success' or the description of error:\n",
        "            - 'iterations_exceeded': if after max_iter iterations of the method x_k still doesn't satisfy\n",
        "                the stopping criterion.\n",
        "            - 'newton_direction_error': in case of failure of solving linear system with Hessian matrix (e.g. non-invertible matrix).\n",
        "            - 'computational_error': in case of getting Infinity or None value during the computations.\n",
        "    history : dictionary of lists or None\n",
        "        Dictionary containing the progress information or None if trace=False.\n",
        "        Dictionary has to be organized as follows:\n",
        "            - history['time'] : list of floats, containing time passed from the start of the method\n",
        "            - history['func'] : list of function values f(x_k) on every step of the algorithm\n",
        "            - history['grad_norm'] : list of values Euclidian norms ||g(x_k)|| of the gradient on every step of the algorithm\n",
        "            - history['x'] : list of np.arrays, containing the trajectory of the algorithm. ONLY STORE IF x.size <= 2\n",
        "    Example:\n",
        "    --------\n",
        "    >> oracle = QuadraticOracle(np.eye(5), np.arange(5))\n",
        "    >> x_opt, message, history = newton(oracle, np.zeros(5), line_search_options={'method': 'Constant', 'c': 1.0})\n",
        "    >> print('Found optimal point: {}'.format(x_opt))\n",
        "       Found optimal point: [ 0.  1.  2.  3.  4.]\n",
        "    \"\"\"\n",
        "    history = defaultdict(list) if trace else None\n",
        "    line_search_tool = get_line_search_tool(line_search_options)\n",
        "    x_k = np.copy(x_0)\n",
        "    norm0 = np.linalg.norm(oracle.grad(x_0)) ** 2\n",
        "    stop = tolerance * norm0\n",
        "    time_start = time()\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        g_k = -oracle.grad(x_k)\n",
        "        norm = np.linalg.norm(g_k)\n",
        "        chk = check(k, x_k, norm, stop, oracle, history, display, trace, time_start)\n",
        "        if chk:\n",
        "            return chk\n",
        "\n",
        "        try:\n",
        "            hess = oracle.hess(x_k)\n",
        "            d_k = scipy.linalg.cho_solve(scipy.linalg.cho_factor(hess), g_k)\n",
        "        except LinAlgError:\n",
        "            return x_k, 'newton_direction_error', history\n",
        "\n",
        "        a_k = line_search_tool.line_search(oracle, x_k, d_k)\n",
        "        x_k = x_k + d_k * a_k\n",
        "\n",
        "    chk = check(max_iter, x_k, np.linalg.norm(oracle.grad(x_k)),\n",
        "                stop, oracle, history, display, trace, time_start)\n",
        "    if chk:\n",
        "        return chk\n",
        "    return x_k, 'iterations_exceeded', history\n",
        "\n",
        "\n",
        "def check(i, x, norm, stop, oracle, history, display, trace, start):\n",
        "    if display:\n",
        "        print(\"iter #{0:4}:\\nx_k = {1}\\nnorm = {2}\".format(i, x, norm))\n",
        "\n",
        "    if trace:\n",
        "        if x.size <= 2:\n",
        "            history['x'].append(x)\n",
        "        history['time'].append((time() - start))\n",
        "        history['func'].append(oracle.func(x))\n",
        "        history['grad_norm'].append(norm)\n",
        "\n",
        "    if norm ** 2 <= stop:\n",
        "        return x, 'success', history"
      ],
      "metadata": {
        "id": "pOoLWOoAUXIx"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}