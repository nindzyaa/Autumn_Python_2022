{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4YDRQatn/exBPDA4oiQ0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nindzyaa/Autumn_Python_2022/blob/main/homework-6/oracles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RtXPyhuxRZne"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.special import expit\n",
        "\n",
        "\n",
        "class BaseSmoothOracle(object):\n",
        "    \"\"\"\n",
        "    Base class for implementation of oracles.\n",
        "    \"\"\"\n",
        "    def func(self, x):\n",
        "        \"\"\"\n",
        "        Computes the value of function at point x.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('Func oracle is not implemented.')\n",
        "\n",
        "    def grad(self, x):\n",
        "        \"\"\"\n",
        "        Computes the gradient at point x.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('Grad oracle is not implemented.')\n",
        "    \n",
        "    def hess(self, x):\n",
        "        \"\"\"\n",
        "        Computes the Hessian matrix at point x.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('Hessian oracle is not implemented.')\n",
        "    \n",
        "    def func_directional(self, x, d, alpha):\n",
        "        \"\"\"\n",
        "        Computes phi(alpha) = f(x + alpha*d).\n",
        "        \"\"\"\n",
        "        return np.squeeze(self.func(x + alpha * d))\n",
        "\n",
        "    def grad_directional(self, x, d, alpha):\n",
        "        \"\"\"\n",
        "        Computes phi'(alpha) = (f(x + alpha*d))'_{alpha}\n",
        "        \"\"\"\n",
        "        return np.squeeze(self.grad(x + alpha * d).dot(d))\n",
        "\n",
        "\n",
        "class QuadraticOracle(BaseSmoothOracle):\n",
        "    \"\"\"\n",
        "    Oracle for quadratic function:\n",
        "       func(x) = 1/2 x^TAx - b^Tx.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, A, b):\n",
        "        if not scipy.sparse.isspmatrix_dia(A) and not np.allclose(A, A.T):\n",
        "            raise ValueError('A should be a symmetric matrix.')\n",
        "        self.A = A\n",
        "        self.b = b\n",
        "\n",
        "    def func(self, x):\n",
        "        return 0.5 * np.dot(self.A.dot(x), x) - self.b.dot(x)\n",
        "\n",
        "    def grad(self, x):\n",
        "        return self.A.dot(x) - self.b\n",
        "\n",
        "    def hess(self, x):\n",
        "        return self.A \n",
        "\n",
        "\n",
        "class LogRegL2Oracle(BaseSmoothOracle):\n",
        "    \"\"\"\n",
        "    Oracle for logistic regression with l2 regularization:\n",
        "         func(x) = 1/m sum_i log(1 + exp(-b_i * a_i^T x)) + regcoef / 2 ||x||_2^2.\n",
        "\n",
        "    Let A and b be parameters of the logistic regression (feature matrix\n",
        "    and labels vector respectively).\n",
        "    For user-friendly interface use create_log_reg_oracle()\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        matvec_Ax : function\n",
        "            Computes matrix-vector product Ax, where x is a vector of size n.\n",
        "        matvec_ATx : function of x\n",
        "            Computes matrix-vector product A^Tx, where x is a vector of size m.\n",
        "        matmat_ATsA : function\n",
        "            Computes matrix-matrix-matrix product A^T * Diag(s) * A,\n",
        "    \"\"\"\n",
        "    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):\n",
        "        self.matvec_Ax = matvec_Ax\n",
        "        self.matvec_ATx = matvec_ATx\n",
        "        self.matmat_ATsA = matmat_ATsA\n",
        "        self.b = b\n",
        "        self.regcoef = regcoef\n",
        "\n",
        "    def func(self, x):\n",
        "        # TODO: Implement\n",
        "        return None\n",
        "\n",
        "    def grad(self, x):\n",
        "        # TODO: Implement\n",
        "        return None\n",
        "\n",
        "    def hess(self, x):\n",
        "        # TODO: Implement\n",
        "        return None\n",
        "\n",
        "\n",
        "class LogRegL2OptimizedOracle(LogRegL2Oracle):\n",
        "    \"\"\"\n",
        "    Oracle for logistic regression with l2 regularization\n",
        "    with optimized *_directional methods (are used in line_search).\n",
        "\n",
        "    For explanation see LogRegL2Oracle.\n",
        "    \"\"\"\n",
        "    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):\n",
        "        super().__init__(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)\n",
        "\n",
        "    def func_directional(self, x, d, alpha):\n",
        "        # TODO: Implement optimized version with pre-computation of Ax and Ad\n",
        "        return None\n",
        "\n",
        "    def grad_directional(self, x, d, alpha):\n",
        "        # TODO: Implement optimized version with pre-computation of Ax and Ad\n",
        "        return None\n",
        "\n",
        "\n",
        "def create_log_reg_oracle(A, b, regcoef, oracle_type='usual'):\n",
        "    \"\"\"\n",
        "    Auxiliary function for creating logistic regression oracles.\n",
        "        `oracle_type` must be either 'usual' or 'optimized'\n",
        "    \"\"\"\n",
        "    matvec_Ax = lambda x: x  # TODO: Implement\n",
        "    matvec_ATx = lambda x: x  # TODO: Implement\n",
        "\n",
        "    def matmat_ATsA(s):\n",
        "        # TODO: Implement\n",
        "        return None\n",
        "\n",
        "    if oracle_type == 'usual':\n",
        "        oracle = LogRegL2Oracle\n",
        "    elif oracle_type == 'optimized':\n",
        "        oracle = LogRegL2OptimizedOracle\n",
        "    else:\n",
        "        raise 'Unknown oracle_type=%s' % oracle_type\n",
        "    return oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)\n",
        "\n",
        "\n",
        "\n",
        "def grad_finite_diff(func, x, eps=1e-8):\n",
        "    \"\"\"\n",
        "    Returns approximation of the gradient using finite differences:\n",
        "        result_i := (f(x + eps * e_i) - f(x)) / eps,\n",
        "        where e_i are coordinate vectors:\n",
        "        e_i = (0, 0, ..., 0, 1, 0, ..., 0)\n",
        "                          >> i <<\n",
        "    \"\"\"\n",
        "    # TODO: Implement numerical estimation of the gradient\n",
        "    return None\n",
        "\n",
        "\n",
        "def hess_finite_diff(func, x, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Returns approximation of the Hessian using finite differences:\n",
        "        result_{ij} := (f(x + eps * e_i + eps * e_j)\n",
        "                               - f(x + eps * e_i) \n",
        "                               - f(x + eps * e_j)\n",
        "                               + f(x)) / eps^2,\n",
        "        where e_i are coordinate vectors:\n",
        "        e_i = (0, 0, ..., 0, 1, 0, ..., 0)\n",
        "                          >> i <<\n",
        "    \"\"\"\n",
        "    # TODO: Implement numerical estimation of the Hessian\n",
        "    return None\n",
        "\n",
        "    n = len(x)\n",
        "    e_j = np.eye(n)\n",
        "    res = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(i, n):\n",
        "            res[i, j] = (func(x + eps * e_j[i] + eps * e_j[j]) \n",
        "                            - func(x + eps * e_j[i]) \n",
        "                            - func(x + eps * e_j[j])\n",
        "                            + func(x)) / eps ** 2\n",
        "            res[j, i] = res[i, j]\n",
        "    return res"
      ]
    }
  ]
}